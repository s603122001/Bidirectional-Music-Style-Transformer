{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from Music_Style_Transfer_master.project.midi_handler import score2midi, midi2score\n",
    "from Music_Style_Transfer_master.project.utils import add_beat, padding_v2, load_model, get_representation, dataset_import\n",
    "from Music_Style_Transfer_master.project.test import style_transfer\n",
    "from Music_Style_Transfer_master.project.model_transformer_relative import _2way_transformer_wavenet_absolute_pitch, _2way_transformer_wavenet_absolute_pitch_new_embedding\n",
    "\n",
    "import tqdm\n",
    "import pypianoroll as pr\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#from keras.models import load_model\n",
    "%matplotlib inline\n",
    "\n",
    "# 33:97 A1~C7  21:109 A0~C8\n",
    "# 7 octave 21:105  A0~G#7  \n",
    "def midi2score(path, beat_resolution, pitch_range=np.arange(21, 109)):\n",
    "    tmp = pr.Multitrack(filename=path, beat_resolution=beat_resolution)   \n",
    "    tmp.binarize()\n",
    "    tmp.assign_constant(1)\n",
    "    # TODO: use variable to handle pitch range\n",
    "    score = tmp.get_merged_pianoroll()[:, pitch_range]\n",
    "    \n",
    "    return score\n",
    "\n",
    "def load_model(path_model):\n",
    "    model = _2way_transformer_wavenet_absolute_pitch_new_embedding(len_context = 8*4*beat_resolution,\n",
    "                                                 n_octave = 7,\n",
    "                                                 size_embedding = 86,\n",
    "                                                 n_transf_layers = 4,\n",
    "                                                 n_transf_downsample = 0,\n",
    "                                                 n_conv_layers = 7,\n",
    "                                                 context_layers = 1)\n",
    "    model.load_weights(path_model)\n",
    "    \n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "beat_resolution = 8\n",
    "pitch_range = 84\n",
    "len_context = beat_resolution*4*8\n",
    "max_len = beat_resolution*4*4\n",
    "\n",
    "path_model = \"../../../scratch/wtl272/projects/Music_style_transfer/runs/run_67/model/model.hdf5\"\n",
    "model = load_model(path_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"output_midi/Results_transformer/RockyRaccoon_bach_model67.mid\"\n",
    "score = midi2score(path, beat_resolution, pitch_range=np.arange(21, 105))\n",
    "score, score_meta = get_representation(score, pitch_range, beat_resolution, len_context)\n",
    "score = (np.array(score[:, :, 2]) > 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_melody = midi2score('input_midi/songs_general/RockyRaccoon_main.mid', beat_resolution, pitch_range=np.arange(21, 105))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "onset_map = []\n",
    "offset_map = []\n",
    "for t in range(len_context, len(score) - len_context):\n",
    "\n",
    "    scaler = np.arange(1, pitch_range + 1)\n",
    "\n",
    "    outputs = np.zeros(shape=(pitch_range, 1))\n",
    "    left_features = (np.array(score[t-len_context:t]) > 0).astype(int)*scaler\n",
    "    left_metas = np.array(score_meta[t-len_context:t])\n",
    "    right_features = (np.array(score[t+1:t+1+len_context]) > 0).astype(int)*scaler\n",
    "    right_metas = np.array(score_meta[t+1:t+1+len_context]) \n",
    "    central_metas = np.array(score_meta[t])\n",
    "    central_metas = np.reshape(central_metas, (1, -1))\n",
    "\n",
    "    i = 0\n",
    "    central_features = (np.array(score[t, :]) > 0).astype(int)\n",
    "    central_features[i:] = 2\n",
    "    central_features = np.reshape(central_features, (1, -1))\n",
    "    \n",
    "    p = model.predict([left_features[None, :, :], left_metas[None, :, :],\n",
    "                               central_features[None, :, :], central_metas[None, :, :],\n",
    "                               right_features[None, :, :], right_metas[None, :, :]]\n",
    "                             )\n",
    "    onset_map.append(p[1][0][-1][84:84*2]*p[2][0][0][84:84*2])\n",
    "    offset_map.append(p[1][0][-1][84*2:]*p[2][0][0][84*2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 68  threshold - >0.05\n",
    "\n",
    "threshold = 0.05\n",
    "o_map = (np.array(onset_map)>threshold).astype(int)\n",
    "score[len_context-1:-(len_context+1)] -=o_map\n",
    "score = (score>0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "score2midi(\"output_midi/Results_onset_fix/RockyRaccoon_bach_model67.mid\", \n",
    "               score[len_context:-(len_context)] , beat_resolution, 120, \n",
    "               pitch_range=np.arange(21, 105), melody_constraint=True, melody=score_melody)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
